{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4758023a-dcd5-4ccf-b3fe-85d6d2879cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  AveragePrice  TotalVolume     plu4046     plu4225    plu4770  \\\n",
      "0   2015-01-04          1.22     40873.28     2819.50    28287.42      49.90   \n",
      "1   2015-01-04          1.79      1373.95       57.42      153.88       0.00   \n",
      "2   2015-01-04          1.00    435021.49   364302.39    23821.16      82.15   \n",
      "3   2015-01-04          1.76      3846.69     1500.15      938.35       0.00   \n",
      "4   2015-01-04          1.08    788025.06    53987.31   552906.04   39995.03   \n",
      "5   2015-01-04          1.29     19137.28     8040.64     6557.47     657.48   \n",
      "6   2015-01-04          1.01     80034.32    44562.12    24964.23    2752.35   \n",
      "7   2015-01-04          1.64      1505.12        1.27     1129.50       0.00   \n",
      "8   2015-01-04          1.02    491738.00     7193.87   396752.18     128.82   \n",
      "9   2015-01-04          1.83      2192.13        8.66      939.43       0.00   \n",
      "10  2015-01-04          1.40    116253.44     3267.97    55693.04     109.55   \n",
      "11  2015-01-04          1.73       379.82        0.00       59.82       0.00   \n",
      "12  2015-01-04          0.93   5777334.90  2843648.26  2267755.26  137479.64   \n",
      "13  2015-01-04          1.24    142349.77   107490.73    25711.96       2.93   \n",
      "14  2015-01-04          1.19    166006.29    29419.03    47220.75   38568.95   \n",
      "\n",
      "    TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
      "0     9716.46    9186.93     529.53         0.0  conventional   \n",
      "1     1162.65    1162.65       0.00         0.0       organic   \n",
      "2    46815.79   16707.15   30108.64         0.0  conventional   \n",
      "3     1408.19    1071.35     336.84         0.0       organic   \n",
      "4   141136.68  137146.07    3990.61         0.0  conventional   \n",
      "5     3881.69    3881.69       0.00         0.0       organic   \n",
      "6     7755.62    6064.30    1691.32         0.0  conventional   \n",
      "7      374.35     186.67     187.68         0.0       organic   \n",
      "8    87663.13   87406.84     256.29         0.0  conventional   \n",
      "9     1244.04    1244.04       0.00         0.0       organic   \n",
      "10   57182.88   57182.88       0.00         0.0  conventional   \n",
      "11     320.00     320.00       0.00         0.0       organic   \n",
      "12  528451.74  477193.38   47882.56      3375.8  conventional   \n",
      "13    9144.15    9144.15       0.00         0.0       organic   \n",
      "14   50797.56   44329.03    6468.53         0.0  conventional   \n",
      "\n",
      "                 region  \n",
      "0                Albany  \n",
      "1                Albany  \n",
      "2               Atlanta  \n",
      "3               Atlanta  \n",
      "4   BaltimoreWashington  \n",
      "5   BaltimoreWashington  \n",
      "6                 Boise  \n",
      "7                 Boise  \n",
      "8                Boston  \n",
      "9                Boston  \n",
      "10     BuffaloRochester  \n",
      "11     BuffaloRochester  \n",
      "12           California  \n",
      "13           California  \n",
      "14            Charlotte  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(df.head(15))\n",
    "else:\n",
    "    print(f\"File {csv_file_name} not found in {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2719155-53ec-407e-aecf-ae1e66bbaf6d",
   "metadata": {},
   "source": [
    "**1. MISSING VALUES**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dddd2-933d-473d-ac87-98727590b778",
   "metadata": {},
   "source": [
    "Checking for missing values in a dataset is a common task in data analysis and can be done easily with libraries in Python, such as pandas. Below is a code snippet using pandas to check for missing values in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b58d3f-31a2-420f-b03f-9dd3fb93c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 rows of the DataFrame:\n",
      "          Date  AveragePrice  TotalVolume     plu4046     plu4225    plu4770  \\\n",
      "0   2015-01-04          1.22     40873.28     2819.50    28287.42      49.90   \n",
      "1   2015-01-04          1.79      1373.95       57.42      153.88       0.00   \n",
      "2   2015-01-04          1.00    435021.49   364302.39    23821.16      82.15   \n",
      "3   2015-01-04          1.76      3846.69     1500.15      938.35       0.00   \n",
      "4   2015-01-04          1.08    788025.06    53987.31   552906.04   39995.03   \n",
      "5   2015-01-04          1.29     19137.28     8040.64     6557.47     657.48   \n",
      "6   2015-01-04          1.01     80034.32    44562.12    24964.23    2752.35   \n",
      "7   2015-01-04          1.64      1505.12        1.27     1129.50       0.00   \n",
      "8   2015-01-04          1.02    491738.00     7193.87   396752.18     128.82   \n",
      "9   2015-01-04          1.83      2192.13        8.66      939.43       0.00   \n",
      "10  2015-01-04          1.40    116253.44     3267.97    55693.04     109.55   \n",
      "11  2015-01-04          1.73       379.82        0.00       59.82       0.00   \n",
      "12  2015-01-04          0.93   5777334.90  2843648.26  2267755.26  137479.64   \n",
      "13  2015-01-04          1.24    142349.77   107490.73    25711.96       2.93   \n",
      "14  2015-01-04          1.19    166006.29    29419.03    47220.75   38568.95   \n",
      "\n",
      "    TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
      "0     9716.46    9186.93     529.53         0.0  conventional   \n",
      "1     1162.65    1162.65       0.00         0.0       organic   \n",
      "2    46815.79   16707.15   30108.64         0.0  conventional   \n",
      "3     1408.19    1071.35     336.84         0.0       organic   \n",
      "4   141136.68  137146.07    3990.61         0.0  conventional   \n",
      "5     3881.69    3881.69       0.00         0.0       organic   \n",
      "6     7755.62    6064.30    1691.32         0.0  conventional   \n",
      "7      374.35     186.67     187.68         0.0       organic   \n",
      "8    87663.13   87406.84     256.29         0.0  conventional   \n",
      "9     1244.04    1244.04       0.00         0.0       organic   \n",
      "10   57182.88   57182.88       0.00         0.0  conventional   \n",
      "11     320.00     320.00       0.00         0.0       organic   \n",
      "12  528451.74  477193.38   47882.56      3375.8  conventional   \n",
      "13    9144.15    9144.15       0.00         0.0       organic   \n",
      "14   50797.56   44329.03    6468.53         0.0  conventional   \n",
      "\n",
      "                 region  \n",
      "0                Albany  \n",
      "1                Albany  \n",
      "2               Atlanta  \n",
      "3               Atlanta  \n",
      "4   BaltimoreWashington  \n",
      "5   BaltimoreWashington  \n",
      "6                 Boise  \n",
      "7                 Boise  \n",
      "8                Boston  \n",
      "9                Boston  \n",
      "10     BuffaloRochester  \n",
      "11     BuffaloRochester  \n",
      "12           California  \n",
      "13           California  \n",
      "14            Charlotte  \n",
      "\n",
      "Missing values in the DataFrame:\n",
      "        Date  AveragePrice  TotalVolume  plu4046  plu4225  plu4770  TotalBags  \\\n",
      "0      False         False        False    False    False    False      False   \n",
      "1      False         False        False    False    False    False      False   \n",
      "2      False         False        False    False    False    False      False   \n",
      "3      False         False        False    False    False    False      False   \n",
      "4      False         False        False    False    False    False      False   \n",
      "...      ...           ...          ...      ...      ...      ...        ...   \n",
      "53410  False         False        False    False    False    False      False   \n",
      "53411  False         False        False    False    False    False      False   \n",
      "53412  False         False        False    False    False    False      False   \n",
      "53413  False         False        False    False    False    False      False   \n",
      "53414  False         False        False    False    False    False      False   \n",
      "\n",
      "       SmallBags  LargeBags  XLargeBags   type  region  \n",
      "0          False      False       False  False   False  \n",
      "1          False      False       False  False   False  \n",
      "2          False      False       False  False   False  \n",
      "3          False      False       False  False   False  \n",
      "4          False      False       False  False   False  \n",
      "...          ...        ...         ...    ...     ...  \n",
      "53410       True       True        True  False   False  \n",
      "53411       True       True        True  False   False  \n",
      "53412       True       True        True  False   False  \n",
      "53413       True       True        True  False   False  \n",
      "53414       True       True        True  False   False  \n",
      "\n",
      "[53415 rows x 12 columns]\n",
      "\n",
      "Count of missing values per column:\n",
      "Date                0\n",
      "AveragePrice        0\n",
      "TotalVolume         0\n",
      "plu4046             0\n",
      "plu4225             0\n",
      "plu4770             0\n",
      "TotalBags           0\n",
      "SmallBags       12390\n",
      "LargeBags       12390\n",
      "XLargeBags      12390\n",
      "type                0\n",
      "region              0\n",
      "dtype: int64\n",
      "\n",
      "Total count of missing values in the DataFrame:\n",
      "37170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(\"First 15 rows of the DataFrame:\")\n",
    "    print(df.head(15))\n",
    "    \n",
    "    # Check for missing values in the DataFrame\n",
    "    print(\"\\nMissing values in the DataFrame:\")\n",
    "    print(df.isna())\n",
    "    \n",
    "    # Count of missing values per column\n",
    "    missing_values_count = df.isna().sum()\n",
    "    print(\"\\nCount of missing values per column:\")\n",
    "    print(missing_values_count)\n",
    "    \n",
    "    # Total count of missing values in the DataFrame\n",
    "    total_missing = df.isna().sum().sum()\n",
    "    print(\"\\nTotal count of missing values in the DataFrame:\")\n",
    "    print(total_missing)\n",
    "    \n",
    "else:\n",
    "    print(f\"File {csv_file_name} not found in {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318e747-fa56-4ef3-b102-5a480138f552",
   "metadata": {},
   "source": [
    "We can see from the outcomes that we have missing values in columns SmallBags,LargeBags and XLargeBags . Each with 12 390 missing values giving a total of 37170 missing values in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89be8bf2-fa3d-4ce4-b068-c6cb99dd2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Remove rows with any missing values\n",
    "df = df.dropna(axis=1)  # Remove columns with any missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f37549-5bbc-47ef-adbf-6baa8b4b323e",
   "metadata": {},
   "source": [
    "Above we dropping rows and columns with missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3361f685-74c0-4ccc-878f-5efb014d5d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 rows of the DataFrame:\n",
      "          Date  AveragePrice  TotalVolume     plu4046     plu4225    plu4770  \\\n",
      "0   2015-01-04          1.22     40873.28     2819.50    28287.42      49.90   \n",
      "1   2015-01-04          1.79      1373.95       57.42      153.88       0.00   \n",
      "2   2015-01-04          1.00    435021.49   364302.39    23821.16      82.15   \n",
      "3   2015-01-04          1.76      3846.69     1500.15      938.35       0.00   \n",
      "4   2015-01-04          1.08    788025.06    53987.31   552906.04   39995.03   \n",
      "5   2015-01-04          1.29     19137.28     8040.64     6557.47     657.48   \n",
      "6   2015-01-04          1.01     80034.32    44562.12    24964.23    2752.35   \n",
      "7   2015-01-04          1.64      1505.12        1.27     1129.50       0.00   \n",
      "8   2015-01-04          1.02    491738.00     7193.87   396752.18     128.82   \n",
      "9   2015-01-04          1.83      2192.13        8.66      939.43       0.00   \n",
      "10  2015-01-04          1.40    116253.44     3267.97    55693.04     109.55   \n",
      "11  2015-01-04          1.73       379.82        0.00       59.82       0.00   \n",
      "12  2015-01-04          0.93   5777334.90  2843648.26  2267755.26  137479.64   \n",
      "13  2015-01-04          1.24    142349.77   107490.73    25711.96       2.93   \n",
      "14  2015-01-04          1.19    166006.29    29419.03    47220.75   38568.95   \n",
      "\n",
      "    TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
      "0     9716.46    9186.93     529.53         0.0  conventional   \n",
      "1     1162.65    1162.65       0.00         0.0       organic   \n",
      "2    46815.79   16707.15   30108.64         0.0  conventional   \n",
      "3     1408.19    1071.35     336.84         0.0       organic   \n",
      "4   141136.68  137146.07    3990.61         0.0  conventional   \n",
      "5     3881.69    3881.69       0.00         0.0       organic   \n",
      "6     7755.62    6064.30    1691.32         0.0  conventional   \n",
      "7      374.35     186.67     187.68         0.0       organic   \n",
      "8    87663.13   87406.84     256.29         0.0  conventional   \n",
      "9     1244.04    1244.04       0.00         0.0       organic   \n",
      "10   57182.88   57182.88       0.00         0.0  conventional   \n",
      "11     320.00     320.00       0.00         0.0       organic   \n",
      "12  528451.74  477193.38   47882.56      3375.8  conventional   \n",
      "13    9144.15    9144.15       0.00         0.0       organic   \n",
      "14   50797.56   44329.03    6468.53         0.0  conventional   \n",
      "\n",
      "                 region  \n",
      "0                Albany  \n",
      "1                Albany  \n",
      "2               Atlanta  \n",
      "3               Atlanta  \n",
      "4   BaltimoreWashington  \n",
      "5   BaltimoreWashington  \n",
      "6                 Boise  \n",
      "7                 Boise  \n",
      "8                Boston  \n",
      "9                Boston  \n",
      "10     BuffaloRochester  \n",
      "11     BuffaloRochester  \n",
      "12           California  \n",
      "13           California  \n",
      "14            Charlotte  \n",
      "\n",
      "Initial missing values in the DataFrame:\n",
      "Date                0\n",
      "AveragePrice        0\n",
      "TotalVolume         0\n",
      "plu4046             0\n",
      "plu4225             0\n",
      "plu4770             0\n",
      "TotalBags           0\n",
      "SmallBags       12390\n",
      "LargeBags       12390\n",
      "XLargeBags      12390\n",
      "type                0\n",
      "region              0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after removing rows and columns:\n",
      "Date            0\n",
      "AveragePrice    0\n",
      "TotalVolume     0\n",
      "plu4046         0\n",
      "plu4225         0\n",
      "plu4770         0\n",
      "TotalBags       0\n",
      "SmallBags       0\n",
      "LargeBags       0\n",
      "XLargeBags      0\n",
      "type            0\n",
      "region          0\n",
      "dtype: int64\n",
      "\n",
      "Total count of missing values after cleaning:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(\"First 15 rows of the DataFrame:\")\n",
    "    print(df.head(15))\n",
    "    \n",
    "    # Check for missing values in the DataFrame\n",
    "    print(\"\\nInitial missing values in the DataFrame:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # Remove rows with any missing values\n",
    "    df_cleaned_rows = df.dropna()\n",
    "    \n",
    "    # Remove columns with any missing values\n",
    "    df_cleaned_cols = df_cleaned_rows.dropna(axis=1)\n",
    "    \n",
    "    # Verify that there are no missing values remaining\n",
    "    print(\"\\nMissing values after removing rows and columns:\")\n",
    "    print(df_cleaned_cols.isna().sum())\n",
    "    \n",
    "    # Total count of missing values after cleaning\n",
    "    total_missing_after_cleaning = df_cleaned_cols.isna().sum().sum()\n",
    "    print(\"\\nTotal count of missing values after cleaning:\")\n",
    "    print(total_missing_after_cleaning)\n",
    "    \n",
    "else:\n",
    "    print(f\"File {csv_file_name} not found in {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028ae39-d92e-41ff-b8ef-bc19f67f6cfe",
   "metadata": {},
   "source": [
    "**2. IMPUTATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917dde7d-0dd9-447f-9087-97b99a49a510",
   "metadata": {},
   "source": [
    "Imputation is a technique used to handle missing data in a dataset. When working with real-world data, it's common to encounter missing values due to various reasons like data entry errors, loss of data, or data not being recorded. Imputation involves filling in these missing values with estimated or calculated values so that the dataset can be used effectively for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6131c2e0-37cb-4ad5-86d9-e5d9bcee180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial missing values in the DataFrame:\n",
      "Date                0\n",
      "AveragePrice        0\n",
      "TotalVolume         0\n",
      "plu4046             0\n",
      "plu4225             0\n",
      "plu4770             0\n",
      "TotalBags           0\n",
      "SmallBags       12390\n",
      "LargeBags       12390\n",
      "XLargeBags      12390\n",
      "type                0\n",
      "region              0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "Date            0\n",
      "AveragePrice    0\n",
      "TotalVolume     0\n",
      "plu4046         0\n",
      "plu4225         0\n",
      "plu4770         0\n",
      "TotalBags       0\n",
      "SmallBags       0\n",
      "LargeBags       0\n",
      "XLargeBags      0\n",
      "type            0\n",
      "region          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Initial missing values in the DataFrame:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    # Impute missing values\n",
    "    # Numeric columns\n",
    "    numeric_columns = df.select_dtypes(include='number').columns\n",
    "    for col in numeric_columns:\n",
    "        # Choose an imputation strategy (e.g., mean)\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    # Categorical columns\n",
    "    categorical_columns = df.select_dtypes(include='object').columns\n",
    "    for col in categorical_columns:\n",
    "        # Choose an imputation strategy (e.g., mode)\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Verify that there are no missing values remaining\n",
    "    print(\"\\nMissing values after imputation:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "else:\n",
    "    print(f\"File {csv_file_name} not found in {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ece169-390e-4866-92c5-7953663806d5",
   "metadata": {},
   "source": [
    "The output indicates that:\n",
    "\n",
    "Initially: There were missing values in SmallBags, LargeBags, and XLargeBags columns.\n",
    "\n",
    "After Imputation: All missing values have been successfully imputed for both numeric and categorical columns, resulting in zero missing values in the DataFrame.\n",
    "\n",
    "Our dataset is now free of missing values, and the imputation process seems to have worked as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00896a58-825e-442b-9d70-efa879e7026b",
   "metadata": {},
   "source": [
    "**3. DUPLICATES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77c7d4-5d2b-4a9d-bf5d-fb966f07119e",
   "metadata": {},
   "source": [
    "**3.1 CHECK FOR DUPLICATE ROWS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fbd4118-8d28-4bb4-b257-64586def1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [Date, AveragePrice, TotalVolume, plu4046, plu4225, plu4770, TotalBags, SmallBags, LargeBags, XLargeBags, type, region]\n",
      "Index: []\n",
      "\n",
      "DataFrame after removing duplicate rows:\n",
      "         Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
      "0  2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
      "1  2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
      "2  2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
      "3  2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
      "4  2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
      "\n",
      "   TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
      "0    9716.46    9186.93     529.53         0.0  conventional   \n",
      "1    1162.65    1162.65       0.00         0.0       organic   \n",
      "2   46815.79   16707.15   30108.64         0.0  conventional   \n",
      "3    1408.19    1071.35     336.84         0.0       organic   \n",
      "4  141136.68  137146.07    3990.61         0.0  conventional   \n",
      "\n",
      "                region  \n",
      "0               Albany  \n",
      "1               Albany  \n",
      "2              Atlanta  \n",
      "3              Atlanta  \n",
      "4  BaltimoreWashington  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicate_rows)\n",
    "    \n",
    "    # Optionally, remove duplicate rows\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    print(\"\\nDataFrame after removing duplicate rows:\")\n",
    "    print(df_no_duplicates.head())\n",
    "\n",
    "else:\n",
    "    print(f\"File {csv_file_name} not found in {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c506b-4a35-4baf-bf5f-43f640c0a83e",
   "metadata": {},
   "source": [
    "Number of duplicate rows: 0: This means that, when considering all columns in the DataFrame, there are no rows that are identical to each other. Every row in your DataFrame is unique when looking at all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b88d7b-0021-42ff-a3b1-8ee0e14582c2",
   "metadata": {},
   "source": [
    "**3.2 CHECK FOR DUPLICATES IN SPECIFIC COLUMNS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfa166a-bac2-40d4-84c9-858492e186d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows based on columns ['Date', 'AveragePrice']: 5487\n",
      "Duplicate rows based on specified columns:\n",
      "             Date  AveragePrice  TotalVolume      plu4046      plu4225  \\\n",
      "26     2015-01-04          1.01    369694.27    121634.27    117865.11   \n",
      "30     2015-01-04          1.02   3382800.12    467259.47   2059657.71   \n",
      "37     2015-01-04          1.22      8938.32      7009.77       671.88   \n",
      "38     2015-01-04          1.02    160130.15      4007.41    118435.79   \n",
      "48     2015-01-04          0.97    540234.22    398670.03     54844.33   \n",
      "...           ...           ...          ...          ...          ...   \n",
      "16947  2017-12-31          0.98  38267341.61  13109617.49  13197460.59   \n",
      "16948  2017-12-31          1.52   1243940.09    120545.76    248123.56   \n",
      "16949  2017-12-31          0.94   6330634.55   1789024.92   2089553.44   \n",
      "16950  2017-12-31          1.48    229477.07     26883.07     53165.30   \n",
      "16951  2017-12-31          0.89    727024.03    347248.16    158559.97   \n",
      "\n",
      "         plu4770    TotalBags   SmallBags   LargeBags  XLargeBags  \\\n",
      "26      74062.76     56132.13    46679.86     1060.51     8391.76   \n",
      "30     318102.38    537780.56   412779.64   111072.91    13928.01   \n",
      "37          0.00      1256.67     1256.67        0.00        0.00   \n",
      "38       1201.50     36485.45    20325.41    16160.04        0.00   \n",
      "48         39.07     86680.79    38505.18    48175.61        0.00   \n",
      "...          ...          ...         ...         ...         ...   \n",
      "16947  644837.54  11315425.99  8092355.20  3098373.91   124696.88   \n",
      "16948    1279.48    873874.64   723304.51   150460.36      109.77   \n",
      "16949   91527.19   2360529.00  1117482.22  1235496.72     7550.06   \n",
      "16950      86.61    149342.09    66254.72    82977.60      109.77   \n",
      "16951    5628.28    215587.62    78402.09   137088.87       96.66   \n",
      "\n",
      "               type            region  \n",
      "26     conventional           Detroit  \n",
      "30     conventional        GreatLakes  \n",
      "37          organic           Houston  \n",
      "38     conventional      Indianapolis  \n",
      "48     conventional             Miami  \n",
      "...             ...               ...  \n",
      "16947  conventional           TotalUS  \n",
      "16948       organic           TotalUS  \n",
      "16949  conventional              West  \n",
      "16950       organic              West  \n",
      "16951  conventional  WestTexNewMexico  \n",
      "\n",
      "[5487 rows x 12 columns]\n",
      "\n",
      "DataFrame after removing duplicates based on specific columns:\n",
      "         Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
      "0  2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
      "1  2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
      "2  2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
      "3  2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
      "4  2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
      "\n",
      "   TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
      "0    9716.46    9186.93     529.53         0.0  conventional   \n",
      "1    1162.65    1162.65       0.00         0.0       organic   \n",
      "2   46815.79   16707.15   30108.64         0.0  conventional   \n",
      "3    1408.19    1071.35     336.84         0.0       organic   \n",
      "4  141136.68  137146.07    3990.61         0.0  conventional   \n",
      "\n",
      "                region  \n",
      "0               Albany  \n",
      "1               Albany  \n",
      "2              Atlanta  \n",
      "3              Atlanta  \n",
      "4  BaltimoreWashington  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Check for duplicate values in specific columns\n",
    "    columns_to_check = ['Date', 'AveragePrice']  # Replace with your columns of interest\n",
    "    duplicate_values = df[df.duplicated(subset=columns_to_check)]\n",
    "    print(f\"Number of duplicate rows based on columns {columns_to_check}: {duplicate_values.shape[0]}\")\n",
    "    print(\"Duplicate rows based on specified columns:\")\n",
    "    print(duplicate_values)\n",
    "    \n",
    "    # Optionally, remove duplicate rows based on specific columns\n",
    "    df_no_duplicates_columns = df.drop_duplicates(subset=columns_to_check)\n",
    "    print(\"\\nDataFrame after removing duplicates based on specific columns:\")\n",
    "    print(df_no_duplicates_columns.head())\n",
    "\n",
    "else:\n",
    "    print(f\"File {csv_file_name} not found in {extracted_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa7176-9274-4553-bc45-27e686eaf9fe",
   "metadata": {},
   "source": [
    "There are 5,487 rows in your DataFrame where the combination of Date and AveragePrice is duplicated. This means that for these 5,487 rows, there are other rows with the same values for these two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f44c3-f5f1-4490-8e31-13d2bd19af02",
   "metadata": {},
   "source": [
    "So below we are removing the duplicate rows based on date and average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0f8c5c-84b2-4274-90a6-a1f9d4ca6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows based on columns ['Date', 'AveragePrice'] before removal: 5487\n",
      "Number of duplicate rows based on columns ['Date', 'AveragePrice'] after removal: 0\n",
      "Number of remaining duplicate rows in the entire DataFrame: 0\n",
      "Remaining duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [Date, AveragePrice, TotalVolume, plu4046, plu4225, plu4770, TotalBags, SmallBags, LargeBags, XLargeBags, type, region]\n",
      "Index: []\n",
      "\n",
      "DataFrame after removing all remaining duplicates:\n",
      "         Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
      "0  2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
      "1  2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
      "2  2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
      "3  2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
      "4  2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
      "\n",
      "   TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
      "0    9716.46    9186.93     529.53         0.0  conventional   \n",
      "1    1162.65    1162.65       0.00         0.0       organic   \n",
      "2   46815.79   16707.15   30108.64         0.0  conventional   \n",
      "3    1408.19    1071.35     336.84         0.0       organic   \n",
      "4  141136.68  137146.07    3990.61         0.0  conventional   \n",
      "\n",
      "                region  \n",
      "0               Albany  \n",
      "1               Albany  \n",
      "2              Atlanta  \n",
      "3              Atlanta  \n",
      "4  BaltimoreWashington  \n",
      "Number of duplicate rows after complete cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already loaded your DataFrame into 'df'\n",
    "\n",
    "# 1. Remove duplicate rows based on 'Date' and 'AveragePrice'\n",
    "df_no_duplicates = df.drop_duplicates(subset=['Date', 'AveragePrice'])\n",
    "\n",
    "# Check if duplicates were removed\n",
    "print(f\"Number of duplicate rows based on columns ['Date', 'AveragePrice'] before removal: {df[df.duplicated(subset=['Date', 'AveragePrice'])].shape[0]}\")\n",
    "print(f\"Number of duplicate rows based on columns ['Date', 'AveragePrice'] after removal: {df_no_duplicates[df_no_duplicates.duplicated(subset=['Date', 'AveragePrice'])].shape[0]}\")\n",
    "\n",
    "# Optionally, check for any remaining duplicates in the entire DataFrame\n",
    "remaining_duplicates = df_no_duplicates[df_no_duplicates.duplicated(keep=False)]\n",
    "print(f\"Number of remaining duplicate rows in the entire DataFrame: {remaining_duplicates.shape[0]}\")\n",
    "print(\"Remaining duplicate rows:\")\n",
    "print(remaining_duplicates)\n",
    "\n",
    "# Optionally, remove any remaining duplicates (if any)\n",
    "df_cleaned = df_no_duplicates.drop_duplicates()\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nDataFrame after removing all remaining duplicates:\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# Check for remaining duplicates after complete cleaning\n",
    "print(f\"Number of duplicate rows after complete cleaning: {df_cleaned[df_cleaned.duplicated(keep=False)].shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e43f37b-9ecb-4e75-9a6a-52f0c8e22f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers detected: 1231\n",
      "Outlier rows:\n",
      "             Date  AveragePrice  TotalVolume      plu4046      plu4225  \\\n",
      "102    2015-01-04      0.950000  31324277.73  12357161.34  13624083.05   \n",
      "210    2015-01-11      1.010000  29063542.75  11544810.53  12134773.38   \n",
      "318    2015-01-18      1.030000  29043458.85  11858139.34  11701947.80   \n",
      "426    2015-01-25      1.040000  28470310.84  12167445.03  10734652.82   \n",
      "444    2015-02-01      0.850000   9032180.67   4794142.14   3460743.80   \n",
      "...           ...           ...          ...          ...          ...   \n",
      "53119  2023-11-19      1.128596  31379171.08  15738262.92   5863099.21   \n",
      "53186  2023-11-26      1.304916   4200357.00   2391331.83    470082.13   \n",
      "53237  2023-11-26      1.127478  28250388.01  13939651.54   5566104.26   \n",
      "53304  2023-12-03      1.066007   5845932.70   3868487.00    591584.26   \n",
      "53355  2023-12-03      1.027284  35575811.85  17911040.02   7463160.04   \n",
      "\n",
      "         plu4770   TotalBags   SmallBags  LargeBags  XLargeBags          type  \\\n",
      "102    844093.32  4498940.02  3585321.58  894945.63    18672.81  conventional   \n",
      "210    866574.66  4517384.18  3783261.16  718333.87    15789.15  conventional   \n",
      "318    831301.90  4652069.81  3873041.26  771093.20     7935.35  conventional   \n",
      "426    768020.05  4800192.94  3978636.90  812924.73     8631.31  conventional   \n",
      "444    177145.49   600149.24   556205.20   41203.20     2740.84  conventional   \n",
      "...          ...         ...         ...        ...         ...           ...   \n",
      "53119  642423.62  7294234.22         NaN        NaN         NaN  conventional   \n",
      "53186  373369.38   834081.63         NaN        NaN         NaN  conventional   \n",
      "53237  597929.14  6478783.93         NaN        NaN         NaN  conventional   \n",
      "53304  340737.02   916388.81         NaN        NaN         NaN  conventional   \n",
      "53355  564714.11  7747176.09         NaN        NaN         NaN  conventional   \n",
      "\n",
      "           region  \n",
      "102       TotalUS  \n",
      "210       TotalUS  \n",
      "318       TotalUS  \n",
      "426       TotalUS  \n",
      "444    California  \n",
      "...           ...  \n",
      "53119     TotalUS  \n",
      "53186  California  \n",
      "53237     TotalUS  \n",
      "53304  California  \n",
      "53355     TotalUS  \n",
      "\n",
      "[1231 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate Z-scores for numerical columns\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "\n",
    "# Define a threshold for what constitutes an anomaly\n",
    "threshold = 3\n",
    "\n",
    "# Identify rows with any Z-score above the threshold\n",
    "outliers = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "print(f\"Number of outliers detected: {outliers.sum()}\")\n",
    "print(\"Outlier rows:\")\n",
    "print(df[outliers])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97451394-6d78-4c94-a0db-65eacb9621b0",
   "metadata": {},
   "source": [
    "**4. ANOMALIESS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004b7f6-266c-4483-9a83-ad3f938df340",
   "metadata": {},
   "source": [
    "Looking for anomalies or inconsistencies in the context of data cleaning is a crucial step in ensuring that your data is accurate, reliable, and ready for analysis. Here’s what this process accomplishes and why it’s important:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72033924-e43a-44ca-81df-7d9245e99890",
   "metadata": {},
   "source": [
    "**4.1. Identifying Data Quality Issues**\n",
    "\n",
    "- **Error Detection:** Anomalies and inconsistencies can indicate data entry errors, measurement mistakes, or other issues that compromise  data quality. Detecting these issues allows you to correct or remove erroneous data, improving the overall accuracy of your dataset.\n",
    "\n",
    "-  **Data Integrity:** Ensuring that the data conforms to expected patterns or ranges helps maintain the integrity of the dataset. This is crucial for making valid inferences and decisions based on the data.\n",
    "\n",
    "**4.2. Improving Model Performance**\n",
    "\n",
    "- **Model Robustness:** Outliers or anomalies can disproportionately influence the results of statistical models or machine learning algorithms. Identifying and addressing these issues helps in building more robust and accurate models.\n",
    "\n",
    "- **Avoiding Bias:** Anomalies can skew results and lead to biased conclusions. By addressing these anomalies, you ensure that the model is not unduly influenced by extreme values.\n",
    "\n",
    "**4.3. Enhancing Data Usability**\n",
    "\n",
    "- **Data Consistency:** Inconsistencies in data formatting, categorization, or ranges can make data difficult to use and analyze. Resolving these inconsistencies ensures that data is standardized and can be used effectively across different analysis tasks.\n",
    "\n",
    "- **4.3.2. Facilitating Analysis:** Clean and consistent data makes it easier to apply various analytical techniques and tools, leading to more reliable and interpretable results.\n",
    "\n",
    "**4.4. Ensuring Accurate Reporting and Insights**\n",
    "\n",
    "- **Reliable Insights:** Accurate and clean data leads to more reliable insights and conclusions. If anomalies are not addressed, they can lead to misleading interpretations and erroneous decisions.\n",
    "\n",
    "- **Effective Communication:** Clean data supports clearer communication of findings to stakeholders, as it reduces the risk of misunderstandings or errors in the reported results.\n",
    "\n",
    "**4.5. Compliance and Standards**\n",
    "\n",
    "- **Regulatory Compliance:** For industries subject to regulations and standards, ensuring that data is free from anomalies and inconsistencies is often a requirement. This helps in maintaining compliance and avoiding potential legal issues.\n",
    "\n",
    "Data Standards: Adhering to data standards and best practices helps in maintaining data quality across different datasets and systems, which is crucial for long-term data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4ee3f4-980f-4573-b7a5-2bad1b70d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking categorical anomalies for column: Date\n",
      "Unique categories: ['2015-01-04' '2015-01-11' '2015-01-18' '2015-01-25' '2015-02-01'\n",
      " '2015-02-08' '2015-02-15' '2015-02-22' '2015-03-01' '2015-03-08'\n",
      " '2015-03-15' '2015-03-22' '2015-03-29' '2015-04-05' '2015-04-12'\n",
      " '2015-04-19' '2015-04-26' '2015-05-03' '2015-05-10' '2015-05-17'\n",
      " '2015-05-24' '2015-05-31' '2015-06-07' '2015-06-14' '2015-06-21'\n",
      " '2015-06-28' '2015-07-05' '2015-07-12' '2015-07-19' '2015-07-26'\n",
      " '2015-08-02' '2015-08-09' '2015-08-16' '2015-08-23' '2015-08-30'\n",
      " '2015-09-06' '2015-09-13' '2015-09-20' '2015-09-27' '2015-10-04'\n",
      " '2015-10-11' '2015-10-18' '2015-10-25' '2015-11-01' '2015-11-08'\n",
      " '2015-11-15' '2015-11-22' '2015-11-29' '2015-12-06' '2015-12-13'\n",
      " '2015-12-20' '2015-12-27' '2016-01-03' '2016-01-10' '2016-01-17'\n",
      " '2016-01-24' '2016-01-31' '2016-02-07' '2016-02-14' '2016-02-21'\n",
      " '2016-02-28' '2016-03-06' '2016-03-13' '2016-03-20' '2016-03-27'\n",
      " '2016-04-03' '2016-04-10' '2016-04-17' '2016-04-24' '2016-05-01'\n",
      " '2016-05-08' '2016-05-15' '2016-05-22' '2016-05-29' '2016-06-05'\n",
      " '2016-06-12' '2016-06-19' '2016-06-26' '2016-07-03' '2016-07-10'\n",
      " '2016-07-17' '2016-07-24' '2016-07-31' '2016-08-07' '2016-08-14'\n",
      " '2016-08-21' '2016-08-28' '2016-09-04' '2016-09-11' '2016-09-18'\n",
      " '2016-09-25' '2016-10-02' '2016-10-09' '2016-10-16' '2016-10-23'\n",
      " '2016-10-30' '2016-11-06' '2016-11-13' '2016-11-20' '2016-11-27'\n",
      " '2016-12-04' '2016-12-11' '2016-12-18' '2016-12-25' '2017-01-01'\n",
      " '2017-01-08' '2017-01-15' '2017-01-22' '2017-01-29' '2017-02-05'\n",
      " '2017-02-12' '2017-02-19' '2017-02-26' '2017-03-05' '2017-03-12'\n",
      " '2017-03-19' '2017-03-26' '2017-04-02' '2017-04-09' '2017-04-16'\n",
      " '2017-04-23' '2017-04-30' '2017-05-07' '2017-05-14' '2017-05-21'\n",
      " '2017-05-28' '2017-06-04' '2017-06-11' '2017-06-18' '2017-06-25'\n",
      " '2017-07-02' '2017-07-09' '2017-07-16' '2017-07-23' '2017-07-30'\n",
      " '2017-08-06' '2017-08-13' '2017-08-20' '2017-08-27' '2017-09-03'\n",
      " '2017-09-10' '2017-09-17' '2017-09-24' '2017-10-01' '2017-10-08'\n",
      " '2017-10-15' '2017-10-22' '2017-10-29' '2017-11-05' '2017-11-12'\n",
      " '2017-11-19' '2017-11-26' '2017-12-03' '2017-12-10' '2017-12-17'\n",
      " '2017-12-24' '2017-12-31' '2018-01-08' '2018-01-14' '2018-01-21'\n",
      " '2018-01-28' '2018-02-04' '2018-02-11' '2018-02-18' '2018-02-25'\n",
      " '2018-03-04' '2018-03-11' '2018-03-18' '2018-03-25' '2018-04-01'\n",
      " '2018-04-08' '2018-04-15' '2018-04-22' '2018-04-29' '2018-05-06'\n",
      " '2018-05-13' '2018-05-20' '2018-05-27' '2018-06-03' '2018-06-10'\n",
      " '2018-06-17' '2018-06-24' '2018-07-01' '2018-07-08' '2018-07-15'\n",
      " '2018-07-22' '2018-07-29' '2018-08-05' '2018-08-12' '2018-08-19'\n",
      " '2018-08-26' '2018-09-02' '2018-09-09' '2018-09-16' '2018-09-23'\n",
      " '2018-09-30' '2018-10-07' '2018-10-14' '2018-10-21' '2018-10-28'\n",
      " '2018-11-04' '2018-11-11' '2018-11-18' '2018-11-25' '2018-12-02'\n",
      " '2018-12-09' '2018-12-16' '2018-12-23' '2018-12-30' '2019-01-07'\n",
      " '2019-01-13' '2019-01-20' '2019-01-27' '2019-02-03' '2019-02-10'\n",
      " '2019-02-17' '2019-02-24' '2019-03-03' '2019-03-10' '2019-03-17'\n",
      " '2019-03-24' '2019-03-31' '2019-04-07' '2019-04-14' '2019-04-21'\n",
      " '2019-04-28' '2019-05-05' '2019-05-12' '2019-05-19' '2019-05-26'\n",
      " '2019-06-02' '2019-06-09' '2019-06-16' '2019-06-23' '2019-06-30'\n",
      " '2019-07-07' '2019-07-14' '2019-07-21' '2019-07-28' '2019-08-04'\n",
      " '2019-08-11' '2019-08-18' '2019-08-25' '2019-09-01' '2019-09-08'\n",
      " '2019-09-15' '2019-09-22' '2019-09-29' '2019-10-06' '2019-10-13'\n",
      " '2019-10-20' '2019-10-27' '2019-11-03' '2019-11-10' '2019-11-17'\n",
      " '2019-11-24' '2019-12-01' '2019-12-08' '2019-12-15' '2019-12-22'\n",
      " '2019-12-29' '2020-01-06' '2020-01-12' '2020-01-19' '2020-01-26'\n",
      " '2020-02-02' '2020-02-09' '2020-02-16' '2020-02-23' '2020-03-01'\n",
      " '2020-03-08' '2020-03-15' '2020-03-22' '2020-03-29' '2020-04-05'\n",
      " '2020-04-12' '2020-04-19' '2020-04-26' '2020-05-03' '2020-05-10'\n",
      " '2020-05-17' '2020-05-24' '2020-05-31' '2020-06-07' '2020-06-14'\n",
      " '2020-06-21' '2020-06-28' '2020-07-05' '2020-07-12' '2020-07-19'\n",
      " '2020-07-26' '2020-08-02' '2020-08-09' '2020-08-16' '2020-08-23'\n",
      " '2020-08-30' '2020-09-06' '2020-09-13' '2020-09-20' '2020-09-27'\n",
      " '2020-10-04' '2020-10-11' '2020-10-18' '2020-10-25' '2020-11-01'\n",
      " '2020-11-08' '2020-11-15' '2020-11-22' '2020-11-29' '2020-12-06'\n",
      " '2020-12-13' '2020-12-20' '2020-12-27' '2021-01-04' '2021-01-10'\n",
      " '2021-01-17' '2021-01-24' '2021-01-31' '2021-02-07' '2021-02-14'\n",
      " '2021-02-21' '2021-02-28' '2021-03-07' '2021-03-14' '2021-03-21'\n",
      " '2021-03-28' '2021-04-04' '2021-04-11' '2021-04-18' '2021-04-25'\n",
      " '2021-05-02' '2021-05-09' '2021-05-16' '2021-05-23' '2021-05-30'\n",
      " '2021-06-06' '2021-06-13' '2021-06-20' '2021-06-27' '2021-07-04'\n",
      " '2021-07-11' '2021-07-18' '2021-07-25' '2021-08-01' '2021-08-08'\n",
      " '2021-08-15' '2021-08-22' '2021-08-29' '2021-09-05' '2021-09-12'\n",
      " '2021-09-19' '2021-09-26' '2021-10-03' '2021-10-10' '2021-10-17'\n",
      " '2021-10-24' '2021-10-31' '2021-11-07' '2021-11-14' '2021-11-21'\n",
      " '2021-11-28' '2021-12-05' '2021-12-12' '2021-12-19' '2021-12-26'\n",
      " '2022-01-02' '2022-01-10' '2022-01-16' '2022-01-23' '2022-01-30'\n",
      " '2022-02-06' '2022-02-13' '2022-02-20' '2022-02-27' '2022-03-06'\n",
      " '2022-03-13' '2022-03-20' '2022-03-27' '2022-04-03' '2022-04-10'\n",
      " '2022-04-17' '2022-04-24' '2022-05-01' '2022-05-08' '2022-05-15'\n",
      " '2022-05-22' '2022-05-29' '2022-06-05' '2022-06-12' '2022-06-19'\n",
      " '2022-06-26' '2022-07-03' '2022-07-10' '2022-07-17' '2022-07-24'\n",
      " '2022-07-31' '2022-08-07' '2022-08-14' '2022-08-21' '2022-08-28'\n",
      " '2022-09-04' '2022-09-11' '2022-09-18' '2022-09-25' '2022-10-02'\n",
      " '2022-10-09' '2022-10-16' '2022-10-23' '2022-10-30' '2022-11-06'\n",
      " '2022-11-13' '2022-11-20' '2022-11-27' '2022-12-04' '2022-12-11'\n",
      " '2022-12-18' '2022-12-25' '2023-01-01' '2023-01-09' '2023-01-15'\n",
      " '2023-01-22' '2023-01-29' '2023-02-05' '2023-02-12' '2023-02-19'\n",
      " '2023-02-26' '2023-03-05' '2023-03-12' '2023-03-19' '2023-03-26'\n",
      " '2023-04-02' '2023-04-09' '2023-04-16' '2023-04-23' '2023-04-30'\n",
      " '2023-05-07' '2023-05-14' '2023-05-21' '2023-05-28' '2023-06-04'\n",
      " '2023-06-11' '2023-06-18' '2023-06-25' '2023-07-02' '2023-07-09'\n",
      " '2023-07-16' '2023-07-23' '2023-07-30' '2023-08-06' '2023-08-13'\n",
      " '2023-08-20' '2023-08-27' '2023-09-03' '2023-09-10' '2023-09-17'\n",
      " '2023-09-24' '2023-10-01' '2023-10-08' '2023-10-15' '2023-10-22'\n",
      " '2023-10-29' '2023-11-05' '2023-11-12' '2023-11-19' '2023-11-26'\n",
      " '2023-12-03']\n",
      "Number of missing values: 0\n",
      "\n",
      "Checking numerical anomalies for column: AveragePrice\n",
      "Min value: 0.44, Max value: 3.440829566\n",
      "Number of outliers detected: 249\n",
      "Outlier rows:\n",
      "3217    2.75\n",
      "3325    2.76\n",
      "3433    2.72\n",
      "3541    2.73\n",
      "3649    2.71\n",
      "Name: AveragePrice, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: TotalVolume\n",
      "Min value: 84.56, Max value: 61034457.1\n",
      "Number of outliers detected: 466\n",
      "Outlier rows:\n",
      "102    31324277.73\n",
      "210    29063542.75\n",
      "318    29043458.85\n",
      "426    28470310.84\n",
      "534    44655461.51\n",
      "Name: TotalVolume, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: plu4046\n",
      "Min value: 0.0, Max value: 25447201.87\n",
      "Number of outliers detected: 483\n",
      "Outlier rows:\n",
      "102    12357161.34\n",
      "210    11544810.53\n",
      "318    11858139.34\n",
      "426    12167445.03\n",
      "444     4794142.14\n",
      "Name: plu4046, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: plu4225\n",
      "Min value: 0.0, Max value: 20470572.61\n",
      "Number of outliers detected: 599\n",
      "Outlier rows:\n",
      "102    13624083.05\n",
      "210    12134773.38\n",
      "318    11701947.80\n",
      "426    10734652.82\n",
      "444     3460743.80\n",
      "Name: plu4225, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: plu4770\n",
      "Min value: 0.0, Max value: 2860025.19\n",
      "Number of outliers detected: 802\n",
      "Outlier rows:\n",
      "102    844093.32\n",
      "210    866574.66\n",
      "318    831301.90\n",
      "426    768020.05\n",
      "462    510153.65\n",
      "Name: plu4770, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: TotalBags\n",
      "Min value: 0.0, Max value: 16298296.29\n",
      "Number of outliers detected: 500\n",
      "Outlier rows:\n",
      "102    4498940.02\n",
      "210    4517384.18\n",
      "318    4652069.81\n",
      "426    4800192.94\n",
      "534    5384427.62\n",
      "Name: TotalBags, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: SmallBags\n",
      "Min value: 0.0, Max value: 12567155.58\n",
      "Number of outliers detected: 285\n",
      "Outlier rows:\n",
      "102    3585321.58\n",
      "210    3783261.16\n",
      "318    3873041.26\n",
      "426    3978636.90\n",
      "534    4216452.03\n",
      "Name: SmallBags, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: LargeBags\n",
      "Min value: 0.0, Max value: 4324231.19\n",
      "Number of outliers detected: 397\n",
      "Outlier rows:\n",
      "102     894945.63\n",
      "210     718333.87\n",
      "318     771093.20\n",
      "426     812924.73\n",
      "534    1121076.47\n",
      "Name: LargeBags, dtype: float64\n",
      "\n",
      "Checking numerical anomalies for column: XLargeBags\n",
      "Min value: 0.0, Max value: 679586.8\n",
      "Number of outliers detected: 302\n",
      "Outlier rows:\n",
      "4422     72218.55\n",
      "5489    108072.79\n",
      "5501    199305.12\n",
      "5537     79085.61\n",
      "5597     84028.58\n",
      "Name: XLargeBags, dtype: float64\n",
      "\n",
      "Checking categorical anomalies for column: type\n",
      "Unique categories: ['conventional' 'organic']\n",
      "Number of missing values: 0\n",
      "\n",
      "Checking categorical anomalies for column: region\n",
      "Unique categories: ['Albany' 'Atlanta' 'BaltimoreWashington' 'Boise' 'Boston'\n",
      " 'BuffaloRochester' 'California' 'Charlotte' 'Chicago' 'CincinnatiDayton'\n",
      " 'Columbus' 'DallasFtWorth' 'Denver' 'Detroit' 'GrandRapids' 'GreatLakes'\n",
      " 'HarrisburgScranton' 'HartfordSpringfield' 'Houston' 'Indianapolis'\n",
      " 'Jacksonville' 'LasVegas' 'LosAngeles' 'Louisville' 'Miami' 'Midsouth'\n",
      " 'Nashville' 'NewOrleans' 'NewYork' 'Northeast' 'NorthernNewEngland'\n",
      " 'Orlando' 'Philadelphia' 'PhoenixTucson' 'Pittsburgh' 'Plains' 'Portland'\n",
      " 'RaleighGreensboro' 'RichmondNorfolk' 'Roanoke' 'Sacramento' 'SanDiego'\n",
      " 'SanFrancisco' 'Seattle' 'SouthCarolina' 'SouthCentral' 'Southeast'\n",
      " 'Spokane' 'StLouis' 'Syracuse' 'Tampa' 'TotalUS' 'West'\n",
      " 'WestTexNewMexico' 'BirminghamMontgomery' 'PeoriaSpringfield'\n",
      " 'Providence' 'Toledo' 'Wichita' 'MiamiFtLauderdale']\n",
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Helper function to check for numerical anomalies\n",
    "def check_numerical_anomalies(df, column):\n",
    "    print(f\"\\nChecking numerical anomalies for column: {column}\")\n",
    "    \n",
    "    # Drop missing values before calculation\n",
    "    column_data = df[column].dropna()\n",
    "    \n",
    "    # Range check (print min/max values)\n",
    "    min_value = column_data.min()\n",
    "    max_value = column_data.max()\n",
    "    print(f\"Min value: {min_value}, Max value: {max_value}\")\n",
    "    \n",
    "    # Statistical outliers using Z-score\n",
    "    z_scores = np.abs(stats.zscore(column_data))\n",
    "    outlier_mask = z_scores > 3\n",
    "    outliers = column_data[outlier_mask]\n",
    "    num_outliers = len(outliers)\n",
    "    print(f\"Number of outliers detected: {num_outliers}\")\n",
    "    \n",
    "    # Display some outliers\n",
    "    if num_outliers > 0:\n",
    "        print(\"Outlier rows:\")\n",
    "        print(outliers.head())\n",
    "\n",
    "# Helper function to check for categorical anomalies\n",
    "def check_categorical_anomalies(df, column):\n",
    "    print(f\"\\nChecking categorical anomalies for column: {column}\")\n",
    "    \n",
    "    # Unique categories\n",
    "    unique_categories = df[column].unique()\n",
    "    print(f\"Unique categories: {unique_categories}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df[column].isna().sum()\n",
    "    print(f\"Number of missing values: {missing_values}\")\n",
    "    \n",
    "    # Display rows with missing values if any\n",
    "    if missing_values > 0:\n",
    "        print(\"Rows with missing values:\")\n",
    "        print(df[df[column].isna()].head())\n",
    "\n",
    "# Helper function to check for date anomalies\n",
    "def check_date_anomalies(df, column):\n",
    "    print(f\"\\nChecking date anomalies for column: {column}\")\n",
    "    \n",
    "    # Convert to datetime and identify invalid dates\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    invalid_dates = df[column].isna().sum()\n",
    "    print(f\"Number of invalid dates detected: {invalid_dates}\")\n",
    "    \n",
    "    # Display rows with invalid dates if any\n",
    "    if invalid_dates > 0:\n",
    "        print(\"Rows with invalid dates:\")\n",
    "        print(df[df[column].isna()].head())\n",
    "\n",
    "# Initial checks for each column\n",
    "for column in df.columns:\n",
    "    dtype = df[column].dtype\n",
    "    if isinstance(dtype, pd.CategoricalDtype) or dtype == 'object':\n",
    "        check_categorical_anomalies(df, column)\n",
    "    elif pd.api.types.is_numeric_dtype(dtype):\n",
    "        check_numerical_anomalies(df, column)\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        check_date_anomalies(df, column)\n",
    "    else:\n",
    "        print(f\"\\nSkipping column {column} of type {dtype} for anomaly checks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36cac0-6993-4c97-bb80-b337b31973af",
   "metadata": {},
   "source": [
    "The results we've shared offer a comprehensive overview of the data quality and the presence of anomalies in our dataset. Now, we investigate Outliers.\n",
    "\n",
    "Reviewing a sample of the outlier data points to understand their context. Are they errors or genuine extreme values? If they're errors, we might need to clean or adjust these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7533aee4-0b23-48ff-9b77-7f4a90f78308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting outliers for TotalVolume:\n",
      "Row 102: 31324277.73\n",
      "Row 210: 29063542.75\n",
      "Row 318: 29043458.85\n",
      "Row 426: 28470310.84\n",
      "Row 534: 44655461.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_dir = r'C:\\Users\\Asus\\Downloads\\archive'\n",
    "\n",
    "# Name of the CSV file\n",
    "csv_file_name = 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv'\n",
    "\n",
    "# Construct the full path to the CSV file\n",
    "csv_file_path = os.path.join(extracted_dir, csv_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Investigate outliers\n",
    "def inspect_outliers(df, column, outlier_rows):\n",
    "    print(f\"Inspecting outliers for {column}:\")\n",
    "    for idx in outlier_rows:\n",
    "        print(f\"Row {idx}: {df.loc[idx, column]}\")\n",
    "\n",
    "# Example: Inspect outliers for TotalVolume\n",
    "outlier_indices = [102, 210, 318, 426, 534]  # Replace with actual outlier indices\n",
    "inspect_outliers(df, 'TotalVolume', outlier_indices)\n",
    "\n",
    "# Visualize with a boxplot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x=df['TotalVolume'])\n",
    "plt.title('TotalVolume Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba129bb-67dd-419e-8ba6-99edb74f0833",
   "metadata": {},
   "source": [
    "Below is a simplified approach to handle the outliers for TotalVolume, we prefer to limit the influence of extreme values without removing them entirely by applying a Capping Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e98968c-2453-4604-8a48-6ecf57a732e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the 99th percentile\n",
    "cap_value = np.percentile(df['TotalVolume'], 99)\n",
    "\n",
    "# Cap values at the 99th percentile\n",
    "df['TotalVolume'] = np.where(df['TotalVolume'] > cap_value, cap_value, df['TotalVolume'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f4c54-a101-493e-be35-ec3efe798621",
   "metadata": {},
   "source": [
    "**5. DATA TRANSFORMATION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcba21e-a116-4e9a-b646-5c96240cb843",
   "metadata": {},
   "source": [
    " **5.1 STANDARDISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839bd09-b377-437b-8f5b-40c21a2890d6",
   "metadata": {},
   "source": [
    " standardization itself is a common preprocessing step specifically to improve model performance and convergence. We ensure that all numerical features are on the same scale, which is crucial for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69ba2edd-3d86-429d-b37f-3959fac4549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns:\n",
      "Index(['AveragePrice', 'TotalVolume', 'plu4046', 'plu4225', 'plu4770',\n",
      "       'TotalBags', 'SmallBags', 'LargeBags', 'XLargeBags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "print(\"Numerical columns:\")\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2249beff-cc60-4ad9-916c-2d0c99828183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized numerical columns:\n",
      "   AveragePrice  TotalVolume   plu4046   plu4225   plu4770  TotalBags  \\\n",
      "0     -0.531426    -0.233714 -0.225939 -0.202971 -0.196760  -0.239478   \n",
      "1      0.918541    -0.244856 -0.228052 -0.232417 -0.197239  -0.249336   \n",
      "2     -1.091063    -0.122538  0.050496 -0.207646 -0.196450  -0.196721   \n",
      "3      0.842227    -0.244159 -0.226948 -0.231595 -0.197239  -0.249053   \n",
      "4     -0.887559    -0.022967 -0.186810  0.346107  0.186971  -0.088018   \n",
      "\n",
      "   SmallBags  LargeBags  XLargeBags  \n",
      "0  -0.166420  -0.152236   -0.120936  \n",
      "1  -0.180516  -0.155774   -0.120936  \n",
      "2  -0.153209   0.045406   -0.120936  \n",
      "3  -0.180677  -0.153523   -0.120936  \n",
      "4   0.058364  -0.129109   -0.120936  \n"
     ]
    }
   ],
   "source": [
    "# List of numerical columns to be standardized\n",
    "numerical_columns = ['AveragePrice', 'TotalVolume', 'plu4046', 'plu4225', 'plu4770',\n",
    "                      'TotalBags', 'SmallBags', 'LargeBags', 'XLargeBags']\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the specified numerical columns\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "print(\"Standardized numerical columns:\")\n",
    "print(df[numerical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d56f7-914d-40b4-ac34-f5fbd8e9b3d8",
   "metadata": {},
   "source": [
    " **5.2 Encode Categorical Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526a2a7-a27c-4b6b-9f90-767e47783f55",
   "metadata": {},
   "source": [
    "Here we convert categorical data into numerical format that can be used in machine learning models. Belo we are doing one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3063a2e5-bcb4-4a84-b5b5-3963e7ae4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns:\n",
      "Index(['Date', 'type', 'region'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check for categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(\"Categorical columns:\")\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77b6212a-051e-4c49-bfa7-dee63fdd98ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with One-Hot Encoded Categorical Columns:\n",
      "   AveragePrice  TotalVolume   plu4046   plu4225   plu4770  TotalBags  \\\n",
      "0     -0.531426    -0.233714 -0.225939 -0.202971 -0.196760  -0.239478   \n",
      "1      0.918541    -0.244856 -0.228052 -0.232417 -0.197239  -0.249336   \n",
      "2     -1.091063    -0.122538  0.050496 -0.207646 -0.196450  -0.196721   \n",
      "3      0.842227    -0.244159 -0.226948 -0.231595 -0.197239  -0.249053   \n",
      "4     -0.887559    -0.022967 -0.186810  0.346107  0.186971  -0.088018   \n",
      "\n",
      "   SmallBags  LargeBags  XLargeBags  Date_2015-01-04  ...  region_Southeast  \\\n",
      "0  -0.166420  -0.152236   -0.120936             True  ...             False   \n",
      "1  -0.180516  -0.155774   -0.120936             True  ...             False   \n",
      "2  -0.153209   0.045406   -0.120936             True  ...             False   \n",
      "3  -0.180677  -0.153523   -0.120936             True  ...             False   \n",
      "4   0.058364  -0.129109   -0.120936             True  ...             False   \n",
      "\n",
      "   region_Spokane  region_StLouis  region_Syracuse  region_Tampa  \\\n",
      "0           False           False            False         False   \n",
      "1           False           False            False         False   \n",
      "2           False           False            False         False   \n",
      "3           False           False            False         False   \n",
      "4           False           False            False         False   \n",
      "\n",
      "   region_Toledo  region_TotalUS  region_West  region_WestTexNewMexico  \\\n",
      "0          False           False        False                    False   \n",
      "1          False           False        False                    False   \n",
      "2          False           False        False                    False   \n",
      "3          False           False        False                    False   \n",
      "4          False           False        False                    False   \n",
      "\n",
      "   region_Wichita  \n",
      "0           False  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "4           False  \n",
      "\n",
      "[5 rows x 537 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of categorical columns\n",
    "categorical_columns = ['Date', 'type', 'region']\n",
    "\n",
    "# Apply One-Hot Encoding to the categorical columns\n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(\"Data with One-Hot Encoded Categorical Columns:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96b29b-8237-4ec4-85a7-581f5f52ff2f",
   "metadata": {},
   "source": [
    "**6. Data Validation and Quality Checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2797c6b-a092-4deb-b5da-62e34aae9298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AveragePrice               0\n",
      "TotalVolume                0\n",
      "plu4046                    0\n",
      "plu4225                    0\n",
      "plu4770                    0\n",
      "                          ..\n",
      "region_Toledo              0\n",
      "region_TotalUS             0\n",
      "region_West                0\n",
      "region_WestTexNewMexico    0\n",
      "region_Wichita             0\n",
      "Length: 537, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2a87e27-4cd1-44fc-98b7-4af5ac927012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AveragePrice               float64\n",
      "TotalVolume                float64\n",
      "plu4046                    float64\n",
      "plu4225                    float64\n",
      "plu4770                    float64\n",
      "                            ...   \n",
      "region_Toledo                 bool\n",
      "region_TotalUS                bool\n",
      "region_West                   bool\n",
      "region_WestTexNewMexico       bool\n",
      "region_Wichita                bool\n",
      "Length: 537, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)  # Check data types of each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2c1d2b7-3eec-4d7a-9aa8-e4e2f370eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dbdedd8-d108-4149-854c-fbde321b6aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "# Example for checking date range\n",
    "print(df['Date_2015-01-04'].min(), df['Date_2015-01-04'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a673cad9-613a-48a3-b751-77c62ac52812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AveragePrice   TotalVolume       plu4046       plu4225       plu4770  \\\n",
      "count  5.341500e+04  5.341500e+04  5.341500e+04  5.341500e+04  5.341500e+04   \n",
      "mean  -5.044235e-16  6.917200e-18  2.554043e-17  9.045569e-18 -5.320923e-18   \n",
      "std    1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   \n",
      "min   -2.515592e+00 -2.452197e-01 -2.280955e-01 -2.325776e-01 -1.972392e-01   \n",
      "25%   -7.881177e-01 -2.406558e-01 -2.275643e-01 -2.303579e-01 -1.972392e-01   \n",
      "50%   -7.354186e-02 -2.112959e-01 -2.169454e-01 -2.142442e-01 -1.963742e-01   \n",
      "75%    6.641609e-01 -1.171174e-01 -1.296046e-01 -1.347019e-01 -1.626585e-01   \n",
      "max    5.117925e+00  1.697064e+01  1.923205e+01  2.119240e+01  2.727745e+01   \n",
      "\n",
      "          TotalBags     SmallBags     LargeBags    XLargeBags  \n",
      "count  5.341500e+04  4.102500e+04  4.102500e+04  4.102500e+04  \n",
      "mean  -4.802133e-17  2.424765e-17 -4.381897e-17 -9.525862e-18  \n",
      "std    1.000009e+00  1.000012e+00  1.000012e+00  1.000012e+00  \n",
      "min   -2.506761e-01 -1.825586e-01 -1.557737e-01 -1.209364e-01  \n",
      "25%   -2.416331e-01 -1.825586e-01 -1.557737e-01 -1.209364e-01  \n",
      "50%   -2.080880e-01 -1.813384e-01 -1.557737e-01 -1.209364e-01  \n",
      "75%   -1.227329e-01 -1.158871e-01 -1.369650e-01 -1.209364e-01  \n",
      "max    1.853294e+01  2.189398e+01  2.873785e+01  2.996416e+01  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3cad94dd-1986-4553-aeab-4078fd9d13dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AveragePrice  TotalVolume    plu4046    plu4225   plu4770  TotalBags  \\\n",
      "102       -1.218253     8.590342   9.221753  14.026707  7.911502   4.934307   \n",
      "210       -1.065625     7.952660   8.600527  12.467961  8.127468   4.955563   \n",
      "318       -1.014749     7.946995   8.840138  12.014955  7.788622   5.110787   \n",
      "426       -0.989311     7.785328   9.076672  11.002562  7.180708   5.281497   \n",
      "444       -1.472633     2.302448   3.438112   3.389518  1.504500   0.440990   \n",
      "...             ...          ...        ...        ...       ...        ...   \n",
      "53119     -0.763939     8.605825  11.807371   5.903880  5.974172   8.155855   \n",
      "53186     -0.315417     0.939544   1.600619   0.259421  3.389515   0.710595   \n",
      "53237     -0.766783     7.723295  10.431925   5.593038  5.546738   7.216057   \n",
      "53304     -0.923155     1.403709   2.730239   0.386588  3.076034   0.805453   \n",
      "53355     -1.021659     9.789565  13.468951   7.578541  5.227659   8.677866   \n",
      "\n",
      "       SmallBags  LargeBags  XLargeBags  Date_2015-01-04  ...  \\\n",
      "102     6.115724   5.824068    0.705703             True  ...   \n",
      "210     6.463441   4.643985    0.578044            False  ...   \n",
      "318     6.621157   4.996512    0.230359            False  ...   \n",
      "426     6.806655   5.276021    0.261169            False  ...   \n",
      "444     0.794519   0.119538    0.000400            False  ...   \n",
      "...          ...        ...         ...              ...  ...   \n",
      "53119        NaN        NaN         NaN            False  ...   \n",
      "53186        NaN        NaN         NaN            False  ...   \n",
      "53237        NaN        NaN         NaN            False  ...   \n",
      "53304        NaN        NaN         NaN            False  ...   \n",
      "53355        NaN        NaN         NaN            False  ...   \n",
      "\n",
      "       region_Southeast  region_Spokane  region_StLouis  region_Syracuse  \\\n",
      "102               False           False           False            False   \n",
      "210               False           False           False            False   \n",
      "318               False           False           False            False   \n",
      "426               False           False           False            False   \n",
      "444               False           False           False            False   \n",
      "...                 ...             ...             ...              ...   \n",
      "53119             False           False           False            False   \n",
      "53186             False           False           False            False   \n",
      "53237             False           False           False            False   \n",
      "53304             False           False           False            False   \n",
      "53355             False           False           False            False   \n",
      "\n",
      "       region_Tampa  region_Toledo  region_TotalUS  region_West  \\\n",
      "102           False          False            True        False   \n",
      "210           False          False            True        False   \n",
      "318           False          False            True        False   \n",
      "426           False          False            True        False   \n",
      "444           False          False           False        False   \n",
      "...             ...            ...             ...          ...   \n",
      "53119         False          False            True        False   \n",
      "53186         False          False           False        False   \n",
      "53237         False          False            True        False   \n",
      "53304         False          False           False        False   \n",
      "53355         False          False            True        False   \n",
      "\n",
      "       region_WestTexNewMexico  region_Wichita  \n",
      "102                      False           False  \n",
      "210                      False           False  \n",
      "318                      False           False  \n",
      "426                      False           False  \n",
      "444                      False           False  \n",
      "...                        ...             ...  \n",
      "53119                    False           False  \n",
      "53186                    False           False  \n",
      "53237                    False           False  \n",
      "53304                    False           False  \n",
      "53355                    False           False  \n",
      "\n",
      "[1231 rows x 537 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the numerical columns\n",
    "numerical_columns = ['AveragePrice', 'TotalVolume', 'plu4046', 'plu4225', 'plu4770',\n",
    "                     'TotalBags', 'SmallBags', 'LargeBags', 'XLargeBags']\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = stats.zscore(df[numerical_columns])\n",
    "\n",
    "# Identify outliers\n",
    "outliers = (abs(z_scores) > 3).any(axis=1)  # Flag rows with any outlier\n",
    "\n",
    "# View outlier rows\n",
    "outlier_rows = df[outliers]\n",
    "print(outlier_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edeec3-5910-436c-bfac-c243f54e473d",
   "metadata": {},
   "source": [
    "##### ****7. PROJECT LANDSCAPE****\n",
    "\n",
    "**7.1. Project Title :**\n",
    "**Avocado Prices Over 5 Years in the USA**\n",
    "\n",
    "**7.2 Purpose**\n",
    "- Visualize avocado price trends and regional differences across the USA.\n",
    "- Explore the correlation between avocado prices and housing affordability for millennials.\n",
    "  \n",
    "**7.3 Dataset Overview**\n",
    "- Source: Hass Avocado Board - Category Data\n",
    "- mTime Period: 2015-2023\n",
    "- Key Datasets:\n",
    "- hass_usa: Country-level sales data.\n",
    "- hass_region: Region-level sales data.\n",
    "- hass: City/sub-region level sales data.\n",
    "  \n",
    "**7.4 Key Variables**\n",
    "- **Date:** Date of observation.\n",
    "- **AveragePrice:** Average price of a single avocado.\n",
    "- **Type:** Conventional or organic.\n",
    "- **Year:** Year of observation.\n",
    "- **Region:** Specific region or city.\n",
    "- **Total Volume:** Total number of avocados sold.\n",
    "- **PLUs:** Codes identifying different types of avocados.\n",
    "  \n",
    " **7.5 Regional Breakdown\n",
    "California**\n",
    "- West (Washington, Oregon, etc.)\n",
    "- Plains (North Dakota, South Dakota, etc.)\n",
    "- South Central (Texas, Oklahoma, etc.)\n",
    "- Southeast (Mississippi, Alabama, etc.)\n",
    "- Midsouth (Kentucky, Tennessee, etc.)\n",
    "- Great Lakes (Wisconsin, Illinois, etc.)\n",
    "- Northeast (Pennsylvania, New York, etc.)\n",
    "  \n",
    "**7.6 Visual Representation Ideas**\n",
    "- **Line Graph:** Show the trend of average avocado prices from 2015 to 2023.\n",
    "- **Bar Chart:** Compare total volume sold across different regions.\n",
    "- **Heat Map:** Visualize price variations across regions over time.\n",
    "- **Scatter Plot:** Analyze the relationship between avocado prices and housing prices.\n",
    "  \n",
    "**7.7 Stakeholders**\n",
    "- **Target Audience:** Millennials, real estate agents, retailers, economic researchers.\n",
    "- **Facilitator/Advisor:** Ereshia Gabier\n",
    "  \n",
    "**7.8 Insights to Explore**\n",
    "- Identify regions with the lowest avocado prices.\n",
    "- Examine trends in avocado consumption and pricing over the years.\n",
    "- Investigate correlations between avocado prices and housing market trends.\n",
    "  \n",
    "**8. Information**\n",
    "\n",
    "**8.1 Dataset Description**\n",
    "\n",
    "- **Data Source:** The dataset was sourced from the Hass Avocado Board, which compiles weekly retail sales data for Hass avocados across the United States.\n",
    "- **Data Collection Method:** Retail scan data collected from various retail channels (grocery, mass, club, drug, dollar, military) reflects actual sales transactions.\n",
    "  \n",
    "**9. Knowledge**\n",
    "\n",
    "**9.1 Market Trends**\n",
    "\n",
    "- **Price Fluctuations:** Understanding how avocado prices have changed over the years can provide insights into consumer behavior, production challenges, and market demand.\n",
    "- **Seasonality:** Prices may vary seasonally based on supply, harvest cycles, and consumer demand (e.g., higher demand in summer for grilling).\n",
    "  \n",
    "**9.2 Consumer Insights**\n",
    "  \n",
    "- **Millennial Preferences:** Analyzing how avocado prices relate to millennials' housing market challenges can reveal broader economic trends and spending behaviors.\n",
    "- **Avocado Toast Culture:** The popularity of avocado toast among millennials may drive demand, influencing prices and availability.\n",
    "  \n",
    "**10. Acknowledgements**\n",
    "\n",
    "\n",
    "Thank the Hass Avocado Board for providing the data.\n",
    "Recognize any collaborators or advisors involved in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b6dd8-819e-4d7d-9044-b75f97f2de3a",
   "metadata": {},
   "source": [
    "**Project Resources**\n",
    "\n",
    "To enhance collaboration and provide easy access to project materials, please find the links to the project management and source code repositories below:\n",
    "\n",
    " - Project Management (Trello): https://trello.com/invite/b/66ebd25879e23adcbf5a327c/ATTIc4aed05ea7d03e5ba4bf8c26254a94d20558D246/avacado-prices-over-5-years-in-usa\n",
    "\n",
    " - Source Code (GitHub): https://github.com/Moraka1952/Individual-Project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19cdfb-0add-498e-97be-f20332dfdcf9",
   "metadata": {},
   "source": [
    "**Notebook Creator: **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
